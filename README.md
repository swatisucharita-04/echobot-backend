# echobot-backend
Backend for EchoBot â€“ Conversational AI using STT, LLM, and TTS

# ğŸ™ï¸ EchoBot â€“ AI Voice Agent

**EchoBot** is an AI-powered voice agent that listens to user speech, understands it, and responds back using **Murf AI** voices.  
It combines **speech-to-text**, **AI reasoning**, and **text-to-speech** to create a seamless conversation experience.

---

## âœ¨ Features
- ğŸ¤ **Voice Recording** â€“ Capture user audio directly in the browser.
- ğŸ“ **Speech-to-Text** â€“ Convert spoken audio into text using **AssemblyAI**.
- ğŸ¤– **AI Understanding** â€“ Process user queries with **Google Gemini AI**.
- ğŸ—£ **Text-to-Speech** â€“ Generate natural-sounding responses with **Murf AI**.
- ğŸŒ **Real-time Web Interface** â€“ Interactive UI with playback support.
- ğŸ”„ **End-to-End Flow** â€“ Fully integrated voice conversation cycle.

---

## ğŸ›  Technologies Used
### Frontend
- HTML5, CSS3, JavaScript (Vanilla)
- Audio recording via MediaRecorder API
- Fetch API for server communication

### Backend
- Python 3.x
- FastAPI (for API endpoints)
- Murf AI API (Text-to-Speech)
- AssemblyAI API (Speech-to-Text)
- Google Gemini API (AI conversation logic)

---

## ğŸ— Architecture

**Flow Diagram:**

[User Voice Input] â†’ [Frontend Recorder] â†’ [Backend API]
â†“ â†“
[AssemblyAI STT] â†’ [Gemini AI Processing] â†’ [Murf AI TTS]
â†“ â†“
[Audio Response] â† [Frontend Playback]


This represents the complete voice agent pipeline from capturing audio to generating responses.

---

## â–¶ï¸ How to Run Locally

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/your-username/echobot.git
cd echobot

2ï¸âƒ£ Install dependencies
bash
pip install -r requirements.txt

3ï¸âƒ£ Set environment variables
Create a .env file in the project root and add:
ASSEMBLYAI_API_KEY=your_assemblyai_api_key
MURF_API_KEY=your_murf_api_key
GEMINI_API_KEY=your_gemini_api_key

4ï¸âƒ£ Start the backend server
bash
uvicorn app:app --reload

â–¶ï¸ One-Command Smoke Test
You can quickly test the full voice agent flow using:

bash
python smoke_test.py

smoke_test.py should:
Record a short audio sample
Process it through STT â†’ LLM â†’ TTS
Play the generated audio

ğŸ›  Troubleshooting
Latency issues: Ensure your internet is stable; close other apps using the microphone.
Microphone permissions: Check OS/browser settings to allow mic access.
Dependencies errors: Run pip install -r requirements.txt and verify .env keys are correct.
Audio not playing: Ensure generated_audio/ folder exists and is writable.

ğŸ“‚ Project Structure
echobot/
30-DAYS-OF-VOICE-AGENTS/
â”‚
â”œâ”€â”€ __pycache__/                 # Python bytecode cache
â”œâ”€â”€ .vscode/                     # VSCode project settings
â”œâ”€â”€ models/                      # AI/ML model scripts or saved models
â”‚
â”œâ”€â”€ node_modules/                # Node.js dependencies
â”‚
â”œâ”€â”€ recordings/                  # Audio recordings
â”‚
â”œâ”€â”€ services/                    # Modular service scripts
â”‚   â”œâ”€â”€ __pycache__/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ llm.py                   # Large Language Model related functions
â”‚   â”œâ”€â”€ stt.py                   # Speech-to-Text services
â”‚   â””â”€â”€ tts.py                   # Text-to-Speech services
â”‚
â”œâ”€â”€ static/                      # Frontend assets
â”‚   â”œâ”€â”€ generated_audio/         # Generated TTS audio files
â”‚   â”œâ”€â”€ JS_audio-processor.js
â”‚   â”œâ”€â”€ JS_audioPlayer.js
â”‚   â”œâ”€â”€ fallback.mp3             # Fallback audio
â”‚   â”œâ”€â”€ favicon.ico
â”‚   â”œâ”€â”€ JS_processor.js          # Audio/voice processing
â”‚   â”œâ”€â”€ JS_script.js             # Main frontend JS logic
â”‚   â”œâ”€â”€ JS_Server.js             # WebSocket or server communication
â”‚   â””â”€â”€ style.css
â”‚
â”œâ”€â”€ templates/                   # HTML templates
â”‚   â””â”€â”€ index.html               # Main UI
â”‚
â”œâ”€â”€ uploads/                     # User-uploaded files
â”‚
â”œâ”€â”€ venv/                        # Python virtual environment
â”‚
â”œâ”€â”€ Python Scripts
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ app.py                   # Main Flask/FastAPI app
â”‚   â”œâ”€â”€ chat.py                  # Chat management
â”‚   â”œâ”€â”€ config.py                # App configuration
â”‚   â”œâ”€â”€ models.py                # ML/AI models
â”‚   â”œâ”€â”€ murff_config.py          # Murf TTS API config
â”‚   â”œâ”€â”€ server.py                # Backend server script
â”‚   â”œâ”€â”€ tempCodeRunnerFile.py    # Temporary VSCode file
â”‚   â”œâ”€â”€ test_env.py              # Environment testing
â”‚   â”œâ”€â”€ test.py                  # Unit tests
â”‚   â””â”€â”€ transcriber.py           # Audio transcription
â”‚
â”œâ”€â”€ Data / Other Files
â”‚   â”œâ”€â”€ assembly_recorded_20250821_161627.wav  # Sample audio recording
â”‚   â”œâ”€â”€ chat_memory.json         # Conversation history
â”‚   â”œâ”€â”€ package.json             # Node.js project config
â”‚   â”œâ”€â”€ package-lock.json        # Node.js dependency lock
â”‚   â”œâ”€â”€ .env                     # Environment variables
â”‚   â”œâ”€â”€ .gitignore               # Files/folders Git ignores
â”‚   â”œâ”€â”€ README.md                # Project documentation
â”‚   â””â”€â”€ requirements.txt         # Python dependencies


How it works

Frontend:
HTML in templates/index.html
JavaScript in static/script.js (handles voice recording, sending data to backend, playing generated audio)
CSS in static/style.css

Backend:
app.py runs FastAPI to serve frontend and handle API requests
murf_config.py connects with Murf TTS API

Data Storage:
uploads/ â†’ Stores user-uploaded audio files
generated_audio/ & static/generated_audio/ â†’ Stores generated voice output
chat_memory.json â†’ Keeps chat or conversation state

Config:
.env â†’ Stores API keys
requirements.txt â†’ Python dependencies